{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Defining Grid Environment"
      ],
      "metadata": {
        "id": "2uL97FyS5257"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Simple grid with predefined Obstacles, starting Position & Goal"
      ],
      "metadata": {
        "id": "xFMHnO6Y5-gM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import random"
      ],
      "metadata": {
        "id": "ikC6Art-DT_D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lNSCHyfX5pr3"
      },
      "outputs": [],
      "source": [
        "# define class grid\n",
        "class GridWorld:\n",
        "    def __init__(self, size=(5, 5), start=(0, 0), goal=(4, 4), obstacles=[]):\n",
        "        self.size = size\n",
        "        self.start = start\n",
        "        self.goal = goal\n",
        "        self.obstacles = obstacles\n",
        "        self.state = self.start\n",
        "\n",
        "    def reset(self):\n",
        "        self.state = self.start\n",
        "        return self.state\n",
        "\n",
        "    def step(self, action):\n",
        "        x, y = self.state\n",
        "        if action == 'up':\n",
        "            x -= 1\n",
        "        elif action == 'down':\n",
        "            x += 1\n",
        "        elif action == 'left':\n",
        "            y -= 1\n",
        "        elif action == 'right':\n",
        "            y += 1\n",
        "        new_state = (max(0, min(x, self.size[0] - 1)), max(0, min(y, self.size[1] - 1)))\n",
        "        if new_state in self.obstacles:\n",
        "            new_state = self.state\n",
        "        self.state = new_state\n",
        "        reward = 1 if new_state == self.goal else -1\n",
        "        return new_state, reward\n",
        "\n",
        "    def is_terminal(self, state):\n",
        "        return state == self.goal"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Defining Q-learning & SARSA Functions"
      ],
      "metadata": {
        "id": "iZRyrIlp6Lvc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Parameter Initialization"
      ],
      "metadata": {
        "id": "EmA8-aqF6_rC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Parameters\n",
        "ALPHA = 0.1\n",
        "GAMMA = 0.9\n",
        "EPSILON = 0.1\n",
        "ACTIONS = ['up', 'down', 'left', 'right']\n",
        "\n",
        "# Q-table\n",
        "def initialize_q_table(env):\n",
        "    q_table = {}\n",
        "    for x in range(env.size[0]):\n",
        "        for y in range(env.size[1]):\n",
        "            q_table[(x, y)] = {action: 0 for action in ACTIONS}\n",
        "    return q_table"
      ],
      "metadata": {
        "id": "UsyZ66JJ6Jet"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Helper Functions"
      ],
      "metadata": {
        "id": "uzzKS0p77CPU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Epsilon-greedy policy\n",
        "def epsilon_greedy(q_table, state):\n",
        "    if random.uniform(0, 1) < EPSILON:\n",
        "        return random.choice(ACTIONS)\n",
        "    else:\n",
        "        return max(q_table[state], key=q_table[state].get)"
      ],
      "metadata": {
        "id": "fDiR39516ZVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# display grid with position, obstacles and goal\n",
        "def print_grid(env, agent_position=None):\n",
        "    grid = [[\" \" for _ in range(env.size[1])] for _ in range(env.size[0])]\n",
        "    x, y = env.goal\n",
        "    grid[x][y] = \"G\"  # Goal\n",
        "    for obs in env.obstacles:\n",
        "        ox, oy = obs\n",
        "        grid[ox][oy] = \"X\"  # Obstacle\n",
        "    if agent_position:\n",
        "        ax, ay = agent_position\n",
        "        grid[ax][ay] = \"A\"  # Agent\n",
        "    for row in grid:\n",
        "        print(\" | \".join(row))\n",
        "    print(\"\\n\")"
      ],
      "metadata": {
        "id": "2-nEnPew6-BN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TD(0) with Q-learning and SARSA"
      ],
      "metadata": {
        "id": "0kUIX3IXD9zI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def q_learning_td0(env, q_table, episodes=500):\n",
        "    for episode in range(episodes):\n",
        "        state = env.reset()\n",
        "\n",
        "        if episode % 100 == 0:\n",
        "            print(f\"\\nEpisode {episode}: Starting new episode...\\n\")\n",
        "            print_grid(env, agent_position=state)\n",
        "\n",
        "        while not env.is_terminal(state):\n",
        "            action = epsilon_greedy(q_table, state)\n",
        "            next_state, reward = env.step(action)\n",
        "            next_action = max(q_table[next_state], key=q_table[next_state].get)\n",
        "            old_value = q_table[state][action]\n",
        "            q_table[state][action] += ALPHA * (reward + GAMMA * q_table[next_state][next_action] - q_table[state][action])\n",
        "\n",
        "            if episode % 100 == 0:\n",
        "                print(f\"Updated Q-value for state {state}, action '{action}': {old_value} -> {q_table[state][action]}\")\n",
        "            state = next_state\n",
        "\n",
        "    return q_table"
      ],
      "metadata": {
        "id": "Sx6R2UcECZk8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# SARSA TD(0)\n",
        "def sarsa_td0(env, q_table, episodes=500):\n",
        "    for episode in range(episodes):\n",
        "        state = env.reset()\n",
        "        action = epsilon_greedy(q_table, state)\n",
        "\n",
        "        if episode % 100 == 0:\n",
        "            print(f\"\\nEpisode {episode}: Starting new episode...\\n\")\n",
        "            print_grid(env, agent_position=state)\n",
        "\n",
        "        while not env.is_terminal(state):\n",
        "            next_state, reward = env.step(action)\n",
        "            next_action = epsilon_greedy(q_table, next_state)\n",
        "            old_value = q_table[state][action]\n",
        "            q_table[state][action] += ALPHA * (reward + GAMMA * q_table[next_state][next_action] - q_table[state][action])\n",
        "\n",
        "            if episode % 100 == 0:\n",
        "                print(f\"Updated Q-value for state {state}, action '{action}': {old_value} -> {q_table[state][action]}\")\n",
        "            state, action = next_state, next_action\n",
        "\n",
        "    return q_table"
      ],
      "metadata": {
        "id": "bd5e1jRG6dwP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TD(N) with Q-learning and SARSA (N=2)"
      ],
      "metadata": {
        "id": "LvEbM-I9D_jW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def q_learning_td2(env, q_table, episodes=500):\n",
        "    for episode in range(episodes):\n",
        "        state = env.reset()\n",
        "        trajectory = []\n",
        "\n",
        "        if episode % 100 == 0:\n",
        "            print(f\"\\nEpisode {episode}: Starting new episode...\\n\")\n",
        "            print_grid(env, agent_position=state)\n",
        "\n",
        "        while not env.is_terminal(state):\n",
        "            action = epsilon_greedy(q_table, state)\n",
        "            next_state, reward = env.step(action)\n",
        "\n",
        "            trajectory.append((state, action, reward))\n",
        "            if len(trajectory) > 2:\n",
        "                trajectory.pop(0)\n",
        "\n",
        "            if len(trajectory) == 2:\n",
        "                (state_t, action_t, reward_t1) = trajectory[0]\n",
        "                (state_t2, action_t2, reward_t2) = trajectory[1]\n",
        "\n",
        "                old_value = q_table[state_t][action_t]\n",
        "                q_table[state_t][action_t] += ALPHA * (\n",
        "                    reward_t1 + GAMMA * reward_t2 + GAMMA**2 * q_table[state_t2][action_t2] - old_value\n",
        "                )\n",
        "\n",
        "                if episode % 100 == 0:\n",
        "                    print(f\"Updated Q-value for state {state_t}, action '{action_t}': {old_value} -> {q_table[state_t][action_t]}\")\n",
        "\n",
        "            state = next_state\n",
        "\n",
        "    return q_table"
      ],
      "metadata": {
        "id": "PtX048ZjDtY8"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# SARSA TD(2)\n",
        "def sarsa_td2(env, q_table, episodes=500):\n",
        "    for episode in range(episodes):\n",
        "        state = env.reset()\n",
        "        action = epsilon_greedy(q_table, state)\n",
        "        trajectory = [(state, action, 0)]\n",
        "        if episode % 100 == 0:\n",
        "            print(f\"\\nEpisode {episode}: Starting new episode...\\n\")\n",
        "            print_grid(env, agent_position=state)\n",
        "        while not env.is_terminal(state):\n",
        "            next_state, reward = env.step(action)\n",
        "            next_action = epsilon_greedy(q_table, next_state)\n",
        "            trajectory.append((next_state, next_action, reward))\n",
        "            if len(trajectory) > 2:\n",
        "                state_t, action_t, _ = trajectory.pop(0)\n",
        "                _, _, reward_t1 = trajectory[0]\n",
        "                state_t2, action_t2, _ = trajectory[1]\n",
        "                old_value = q_table[state_t][action_t]\n",
        "                q_table[state_t][action_t] += ALPHA * (reward_t1 + GAMMA * reward + GAMMA**2 * q_table[state_t2][action_t2] - old_value)\n",
        "                if episode % 100 == 0:\n",
        "                    print(f\"Updated Q-value for state {state_t}, action '{action_t}': {old_value} -> {q_table[state_t][action_t]}\")\n",
        "            state, action = next_state, next_action\n",
        "    return q_table"
      ],
      "metadata": {
        "id": "62YlBVBT6iUW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Path post Training"
      ],
      "metadata": {
        "id": "U0OzEU_FEMJJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def display_agent_path(env, q_table):\n",
        "    state = env.reset()\n",
        "    path = [state]\n",
        "\n",
        "    print(\"\\nAgent's path to goal after training:\\n\")\n",
        "    while not env.is_terminal(state):\n",
        "        print_grid(env, agent_position=state)\n",
        "\n",
        "        # choose best action based on learned Q-table\n",
        "        action = max(q_table[state], key=q_table[state].get)\n",
        "        state, _ = env.step(action)\n",
        "        path.append(state)\n",
        "\n",
        "        # end path if stuck in a loop (to handle bad policies)\n",
        "        if len(path) > 100:  # Arbitrary loop detection limit\n",
        "            print(\"Loop detected, stopping path display.\")\n",
        "            break\n",
        "\n",
        "    print(\"Final Path:\", path)\n",
        "    print_grid(env, agent_position=state)"
      ],
      "metadata": {
        "id": "FvgSsth_7m2Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Deploying the Agent"
      ],
      "metadata": {
        "id": "yHZkZGJJ7F2k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "env = GridWorld(size=(5, 5), start=(0, 0), goal=(4, 4), obstacles=[(1, 1), (2, 2), (3, 3)])\n",
        "q_table = initialize_q_table(env)"
      ],
      "metadata": {
        "id": "u00j9bCl6lAr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in q_table.items():\n",
        "  print(i)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tWxsi6vAEUWv",
        "outputId": "17ff0ec1-9fc6-4a6d-884e-a55094cdf165"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "((0, 0), {'up': 0, 'down': 0, 'left': 0, 'right': 0})\n",
            "((0, 1), {'up': 0, 'down': 0, 'left': 0, 'right': 0})\n",
            "((0, 2), {'up': 0, 'down': 0, 'left': 0, 'right': 0})\n",
            "((0, 3), {'up': 0, 'down': 0, 'left': 0, 'right': 0})\n",
            "((0, 4), {'up': 0, 'down': 0, 'left': 0, 'right': 0})\n",
            "((1, 0), {'up': 0, 'down': 0, 'left': 0, 'right': 0})\n",
            "((1, 1), {'up': 0, 'down': 0, 'left': 0, 'right': 0})\n",
            "((1, 2), {'up': 0, 'down': 0, 'left': 0, 'right': 0})\n",
            "((1, 3), {'up': 0, 'down': 0, 'left': 0, 'right': 0})\n",
            "((1, 4), {'up': 0, 'down': 0, 'left': 0, 'right': 0})\n",
            "((2, 0), {'up': 0, 'down': 0, 'left': 0, 'right': 0})\n",
            "((2, 1), {'up': 0, 'down': 0, 'left': 0, 'right': 0})\n",
            "((2, 2), {'up': 0, 'down': 0, 'left': 0, 'right': 0})\n",
            "((2, 3), {'up': 0, 'down': 0, 'left': 0, 'right': 0})\n",
            "((2, 4), {'up': 0, 'down': 0, 'left': 0, 'right': 0})\n",
            "((3, 0), {'up': 0, 'down': 0, 'left': 0, 'right': 0})\n",
            "((3, 1), {'up': 0, 'down': 0, 'left': 0, 'right': 0})\n",
            "((3, 2), {'up': 0, 'down': 0, 'left': 0, 'right': 0})\n",
            "((3, 3), {'up': 0, 'down': 0, 'left': 0, 'right': 0})\n",
            "((3, 4), {'up': 0, 'down': 0, 'left': 0, 'right': 0})\n",
            "((4, 0), {'up': 0, 'down': 0, 'left': 0, 'right': 0})\n",
            "((4, 1), {'up': 0, 'down': 0, 'left': 0, 'right': 0})\n",
            "((4, 2), {'up': 0, 'down': 0, 'left': 0, 'right': 0})\n",
            "((4, 3), {'up': 0, 'down': 0, 'left': 0, 'right': 0})\n",
            "((4, 4), {'up': 0, 'down': 0, 'left': 0, 'right': 0})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train with Q-learning TD(0)"
      ],
      "metadata": {
        "id": "lM6deLcEHFYl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nTraining with Q-learning TD(0):\\n\")\n",
        "q_learning_td0_q_table = q_learning_td0(env, q_table, episodes=500)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VaT84t936rxO",
        "outputId": "1cdd0485-9c03-4921-afbd-c8f757ce8a55"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training with Q-learning TD(0):\n",
            "\n",
            "\n",
            "Episode 0: Starting new episode...\n",
            "\n",
            "A |   |   |   |  \n",
            "  | X |   |   |  \n",
            "  |   | X |   |  \n",
            "  |   |   | X |  \n",
            "  |   |   |   | G\n",
            "\n",
            "\n",
            "Updated Q-value for state (0, 0), action 'up': 0 -> -0.1\n",
            "Updated Q-value for state (0, 0), action 'down': 0 -> -0.1\n",
            "Updated Q-value for state (1, 0), action 'up': 0 -> -0.1\n",
            "Updated Q-value for state (0, 0), action 'left': 0 -> -0.1\n",
            "Updated Q-value for state (0, 0), action 'left': -0.1 -> -0.19\n",
            "Updated Q-value for state (0, 0), action 'right': 0 -> -0.1\n",
            "Updated Q-value for state (0, 1), action 'up': 0 -> -0.1\n",
            "Updated Q-value for state (0, 1), action 'left': 0 -> -0.10900000000000001\n",
            "Updated Q-value for state (0, 0), action 'up': -0.1 -> -0.199\n",
            "Updated Q-value for state (0, 0), action 'down': -0.1 -> -0.19\n",
            "Updated Q-value for state (1, 0), action 'down': 0 -> -0.1\n",
            "Updated Q-value for state (2, 0), action 'up': 0 -> -0.1\n",
            "Updated Q-value for state (1, 0), action 'left': 0 -> -0.1\n",
            "Updated Q-value for state (1, 0), action 'right': 0 -> -0.1\n",
            "Updated Q-value for state (1, 0), action 'up': -0.1 -> -0.199\n",
            "Updated Q-value for state (0, 0), action 'right': -0.1 -> -0.19\n",
            "Updated Q-value for state (0, 1), action 'down': 0 -> -0.1\n",
            "Updated Q-value for state (0, 1), action 'right': 0 -> -0.1\n",
            "Updated Q-value for state (0, 2), action 'down': 0 -> -0.1\n",
            "Updated Q-value for state (1, 2), action 'up': 0 -> -0.1\n",
            "Updated Q-value for state (0, 2), action 'up': 0 -> -0.1\n",
            "Updated Q-value for state (0, 2), action 'left': 0 -> -0.10900000000000001\n",
            "Updated Q-value for state (0, 1), action 'up': -0.1 -> -0.199\n",
            "Updated Q-value for state (0, 1), action 'down': -0.1 -> -0.199\n",
            "Updated Q-value for state (0, 1), action 'down': -0.199 -> -0.2881\n",
            "Updated Q-value for state (0, 1), action 'right': -0.1 -> -0.19\n",
            "Updated Q-value for state (0, 2), action 'right': 0 -> -0.1\n",
            "Updated Q-value for state (0, 3), action 'up': 0 -> -0.1\n",
            "Updated Q-value for state (0, 3), action 'down': 0 -> -0.1\n",
            "Updated Q-value for state (1, 3), action 'up': 0 -> -0.1\n",
            "Updated Q-value for state (0, 3), action 'left': 0 -> -0.10900000000000001\n",
            "Updated Q-value for state (0, 2), action 'up': -0.1 -> -0.199\n",
            "Updated Q-value for state (0, 2), action 'right': -0.1 -> -0.19\n",
            "Updated Q-value for state (0, 3), action 'right': 0 -> -0.1\n",
            "Updated Q-value for state (0, 4), action 'up': 0 -> -0.1\n",
            "Updated Q-value for state (0, 4), action 'down': 0 -> -0.1\n",
            "Updated Q-value for state (1, 4), action 'up': 0 -> -0.1\n",
            "Updated Q-value for state (0, 4), action 'left': 0 -> -0.10900000000000001\n",
            "Updated Q-value for state (0, 3), action 'up': -0.1 -> -0.199\n",
            "Updated Q-value for state (0, 3), action 'down': -0.1 -> -0.19\n",
            "Updated Q-value for state (1, 3), action 'down': 0 -> -0.1\n",
            "Updated Q-value for state (2, 3), action 'up': 0 -> -0.1\n",
            "Updated Q-value for state (1, 3), action 'left': 0 -> -0.1\n",
            "Updated Q-value for state (1, 2), action 'down': 0 -> -0.1\n",
            "Updated Q-value for state (1, 2), action 'left': 0 -> -0.1\n",
            "Updated Q-value for state (1, 2), action 'right': 0 -> -0.1\n",
            "Updated Q-value for state (1, 3), action 'right': 0 -> -0.1\n",
            "Updated Q-value for state (1, 4), action 'down': 0 -> -0.1\n",
            "Updated Q-value for state (2, 4), action 'up': 0 -> -0.1\n",
            "Updated Q-value for state (1, 4), action 'left': 0 -> -0.10900000000000001\n",
            "Updated Q-value for state (1, 3), action 'up': -0.1 -> -0.199\n",
            "Updated Q-value for state (0, 3), action 'right': -0.1 -> -0.19\n",
            "Updated Q-value for state (0, 4), action 'right': 0 -> -0.1\n",
            "Updated Q-value for state (0, 4), action 'up': -0.1 -> -0.199\n",
            "Updated Q-value for state (0, 4), action 'down': -0.1 -> -0.19\n",
            "Updated Q-value for state (1, 4), action 'right': 0 -> -0.1\n",
            "Updated Q-value for state (1, 4), action 'up': -0.1 -> -0.199\n",
            "Updated Q-value for state (0, 4), action 'right': -0.1 -> -0.199\n",
            "Updated Q-value for state (0, 4), action 'left': -0.10900000000000001 -> -0.20791000000000004\n",
            "Updated Q-value for state (0, 3), action 'left': -0.10900000000000001 -> -0.20710000000000003\n",
            "Updated Q-value for state (0, 2), action 'down': -0.1 -> -0.199\n",
            "Updated Q-value for state (1, 2), action 'up': -0.1 -> -0.19981000000000002\n",
            "Updated Q-value for state (0, 2), action 'left': -0.10900000000000001 -> -0.20791000000000004\n",
            "Updated Q-value for state (0, 1), action 'left': -0.10900000000000001 -> -0.21520000000000003\n",
            "Updated Q-value for state (0, 0), action 'down': -0.19 -> -0.28\n",
            "Updated Q-value for state (1, 0), action 'down': -0.1 -> -0.19\n",
            "Updated Q-value for state (2, 0), action 'down': 0 -> -0.1\n",
            "Updated Q-value for state (3, 0), action 'up': 0 -> -0.1\n",
            "Updated Q-value for state (2, 0), action 'left': 0 -> -0.1\n",
            "Updated Q-value for state (2, 0), action 'right': 0 -> -0.1\n",
            "Updated Q-value for state (2, 1), action 'up': 0 -> -0.1\n",
            "Updated Q-value for state (2, 1), action 'down': 0 -> -0.1\n",
            "Updated Q-value for state (3, 1), action 'up': 0 -> -0.1\n",
            "Updated Q-value for state (2, 1), action 'right': 0 -> -0.1\n",
            "Updated Q-value for state (2, 1), action 'left': 0 -> -0.10900000000000001\n",
            "Updated Q-value for state (2, 0), action 'up': -0.1 -> -0.199\n",
            "Updated Q-value for state (1, 0), action 'left': -0.1 -> -0.199\n",
            "Updated Q-value for state (1, 0), action 'right': -0.1 -> -0.199\n",
            "Updated Q-value for state (1, 0), action 'down': -0.19 -> -0.28\n",
            "Updated Q-value for state (2, 0), action 'down': -0.1 -> -0.19\n",
            "Updated Q-value for state (3, 0), action 'down': 0 -> -0.1\n",
            "Updated Q-value for state (4, 0), action 'up': 0 -> -0.1\n",
            "Updated Q-value for state (3, 0), action 'left': 0 -> -0.1\n",
            "Updated Q-value for state (3, 0), action 'right': 0 -> -0.1\n",
            "Updated Q-value for state (3, 1), action 'down': 0 -> -0.1\n",
            "Updated Q-value for state (4, 1), action 'up': 0 -> -0.1\n",
            "Updated Q-value for state (3, 1), action 'left': 0 -> -0.10900000000000001\n",
            "Updated Q-value for state (3, 0), action 'up': -0.1 -> -0.199\n",
            "Updated Q-value for state (2, 0), action 'left': -0.1 -> -0.199\n",
            "Updated Q-value for state (2, 0), action 'right': -0.1 -> -0.199\n",
            "Updated Q-value for state (2, 1), action 'up': -0.1 -> -0.199\n",
            "Updated Q-value for state (2, 1), action 'down': -0.1 -> -0.19\n",
            "Updated Q-value for state (3, 1), action 'right': 0 -> -0.1\n",
            "Updated Q-value for state (3, 2), action 'up': 0 -> -0.1\n",
            "Updated Q-value for state (3, 2), action 'down': 0 -> -0.1\n",
            "Updated Q-value for state (4, 2), action 'up': 0 -> -0.1\n",
            "Updated Q-value for state (3, 2), action 'left': 0 -> -0.10900000000000001\n",
            "Updated Q-value for state (3, 1), action 'up': -0.1 -> -0.199\n",
            "Updated Q-value for state (2, 1), action 'right': -0.1 -> -0.199\n",
            "Updated Q-value for state (2, 1), action 'left': -0.10900000000000001 -> -0.21520000000000003\n",
            "Updated Q-value for state (2, 0), action 'down': -0.19 -> -0.28\n",
            "Updated Q-value for state (3, 0), action 'down': -0.1 -> -0.19\n",
            "Updated Q-value for state (4, 0), action 'down': 0 -> -0.1\n",
            "Updated Q-value for state (4, 0), action 'left': 0 -> -0.1\n",
            "Updated Q-value for state (4, 0), action 'right': 0 -> -0.1\n",
            "Updated Q-value for state (4, 1), action 'down': 0 -> -0.1\n",
            "Updated Q-value for state (4, 1), action 'left': 0 -> -0.10900000000000001\n",
            "Updated Q-value for state (4, 0), action 'up': -0.1 -> -0.199\n",
            "Updated Q-value for state (3, 0), action 'left': -0.1 -> -0.199\n",
            "Updated Q-value for state (3, 0), action 'right': -0.1 -> -0.199\n",
            "Updated Q-value for state (3, 1), action 'down': -0.1 -> -0.19\n",
            "Updated Q-value for state (4, 1), action 'right': 0 -> -0.1\n",
            "Updated Q-value for state (4, 2), action 'up': -0.1 -> -0.19\n",
            "Updated Q-value for state (3, 2), action 'right': 0 -> -0.1\n",
            "Updated Q-value for state (3, 2), action 'up': -0.1 -> -0.199\n",
            "Updated Q-value for state (3, 2), action 'down': -0.1 -> -0.19\n",
            "Updated Q-value for state (4, 2), action 'down': 0 -> -0.1\n",
            "Updated Q-value for state (4, 2), action 'left': 0 -> -0.10900000000000001\n",
            "Updated Q-value for state (4, 1), action 'up': -0.1 -> -0.199\n",
            "Updated Q-value for state (3, 1), action 'right': -0.1 -> -0.199\n",
            "Updated Q-value for state (3, 2), action 'right': -0.1 -> -0.199\n",
            "Updated Q-value for state (3, 2), action 'left': -0.10900000000000001 -> -0.20791000000000004\n",
            "Updated Q-value for state (3, 1), action 'left': -0.10900000000000001 -> -0.21520000000000003\n",
            "Updated Q-value for state (3, 0), action 'down': -0.19 -> -0.28\n",
            "Updated Q-value for state (4, 0), action 'down': -0.1 -> -0.199\n",
            "Updated Q-value for state (4, 0), action 'left': -0.1 -> -0.199\n",
            "Updated Q-value for state (4, 0), action 'right': -0.1 -> -0.199\n",
            "Updated Q-value for state (4, 1), action 'down': -0.1 -> -0.199\n",
            "Updated Q-value for state (4, 1), action 'right': -0.1 -> -0.19\n",
            "Updated Q-value for state (4, 2), action 'right': 0 -> -0.1\n",
            "Updated Q-value for state (4, 3), action 'up': 0 -> -0.1\n",
            "Updated Q-value for state (4, 3), action 'down': 0 -> -0.1\n",
            "Updated Q-value for state (4, 3), action 'up': -0.1 -> -0.19\n",
            "Updated Q-value for state (4, 3), action 'left': 0 -> -0.10900000000000001\n",
            "Updated Q-value for state (4, 2), action 'down': -0.1 -> -0.199\n",
            "Updated Q-value for state (4, 2), action 'right': -0.1 -> -0.19\n",
            "Updated Q-value for state (4, 3), action 'right': 0 -> 0.1\n",
            "\n",
            "Episode 100: Starting new episode...\n",
            "\n",
            "A |   |   |   |  \n",
            "  | X |   |   |  \n",
            "  |   | X |   |  \n",
            "  |   |   | X |  \n",
            "  |   |   |   | G\n",
            "\n",
            "\n",
            "Updated Q-value for state (0, 0), action 'right': -3.8003270091598775 -> -3.81979831648359\n",
            "Updated Q-value for state (0, 1), action 'right': -3.3278223137744463 -> -3.3461290568136866\n",
            "Updated Q-value for state (0, 2), action 'right': -2.7898774935187216 -> -2.8138582848740286\n",
            "Updated Q-value for state (0, 3), action 'down': -2.255206007857546 -> -2.2940728585164303\n",
            "Updated Q-value for state (1, 3), action 'down': -1.826527238273767 -> -1.844751372973177\n",
            "Updated Q-value for state (2, 3), action 'right': -1.1208539836309646 -> -1.120087784111162\n",
            "Updated Q-value for state (2, 4), action 'right': -0.490099501 -> -0.552408749743294\n",
            "Updated Q-value for state (2, 4), action 'down': -0.12576887603660056 -> -0.1236558282015993\n",
            "Updated Q-value for state (3, 4), action 'right': 0 -> -0.0104638397686588\n",
            "Updated Q-value for state (3, 4), action 'down': 0.9948462247926799 -> 0.995361602313412\n",
            "\n",
            "Episode 200: Starting new episode...\n",
            "\n",
            "A |   |   |   |  \n",
            "  | X |   |   |  \n",
            "  |   | X |   |  \n",
            "  |   |   | X |  \n",
            "  |   |   |   | G\n",
            "\n",
            "\n",
            "Updated Q-value for state (0, 0), action 'right': -4.461673558304126 -> -4.468326229498485\n",
            "Updated Q-value for state (0, 1), action 'right': -3.920222522497472 -> -3.926239774314589\n",
            "Updated Q-value for state (0, 2), action 'down': -3.311550045187385 -> -3.324100918172315\n",
            "Updated Q-value for state (1, 2), action 'right': -2.7078430833740956 -> -2.7145661392130735\n",
            "Updated Q-value for state (1, 3), action 'right': -1.972304046404303 -> -1.9734361109918763\n",
            "Updated Q-value for state (1, 4), action 'down': -1.0929163247555964 -> -1.0926464221604668\n",
            "Updated Q-value for state (2, 4), action 'down': -0.10024144311588817 -> -0.10021945027760926\n",
            "Updated Q-value for state (3, 4), action 'down': 0.9999760947410011 -> 0.999978485266901\n",
            "\n",
            "Episode 300: Starting new episode...\n",
            "\n",
            "A |   |   |   |  \n",
            "  | X |   |   |  \n",
            "  |   | X |   |  \n",
            "  |   |   | X |  \n",
            "  |   |   |   | G\n",
            "\n",
            "\n",
            "Updated Q-value for state (0, 0), action 'down': -4.67197890528882 -> -4.67401348617339\n",
            "Updated Q-value for state (1, 0), action 'down': -4.102583015705024 -> -4.1040392080933525\n",
            "Updated Q-value for state (2, 0), action 'down': -3.4634943773203393 -> -3.4656432572123954\n",
            "Updated Q-value for state (3, 0), action 'down': -2.7610924180454437 -> -2.7630946913675554\n",
            "Updated Q-value for state (4, 0), action 'right': -1.979016834740624 -> -1.9792204843896974\n",
            "Updated Q-value for state (4, 1), action 'right': -1.0900592569237306 -> -1.0900535368727795\n",
            "Updated Q-value for state (4, 2), action 'right': -0.10000228490468753 -> -0.10000207162438313\n",
            "Updated Q-value for state (4, 3), action 'right': 0.9999998309981739 -> 0.9999998478983565\n",
            "\n",
            "Episode 400: Starting new episode...\n",
            "\n",
            "A |   |   |   |  \n",
            "  | X |   |   |  \n",
            "  |   | X |   |  \n",
            "  |   |   | X |  \n",
            "  |   |   |   | G\n",
            "\n",
            "\n",
            "Updated Q-value for state (0, 0), action 'down': -4.724764917736346 -> -4.725199296096637\n",
            "Updated Q-value for state (1, 0), action 'down': -4.1434541125991675 -> -4.143804324717031\n",
            "Updated Q-value for state (2, 0), action 'down': -3.4966180375308986 -> -3.497022568001787\n",
            "Updated Q-value for state (3, 0), action 'right': -2.778514824710869 -> -2.7789535555693115\n",
            "Updated Q-value for state (3, 1), action 'down': -1.9810023703281054 -> -1.981002164527688\n",
            "Updated Q-value for state (4, 1), action 'right': -1.0900003470265918 -> -1.0900003139283099\n",
            "Updated Q-value for state (4, 2), action 'right': -0.10000001782641388 -> -0.10000001613087223\n",
            "Updated Q-value for state (4, 3), action 'right': 0.9999999990322251 -> 0.9999999991290026\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train with SARSA TD(0)\n",
        "print(\"\\nTraining with SARSA TD(0):\\n\")\n",
        "sarsa_td0_q_table = sarsa_td0(env, q_table, episodes=500)"
      ],
      "metadata": {
        "id": "3R-NBmSDEsC1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "124037f9-e2a4-44bb-e9f5-ed5dea9d28e6"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training with SARSA TD(0):\n",
            "\n",
            "\n",
            "Episode 0: Starting new episode...\n",
            "\n",
            "A |   |   |   |  \n",
            "  | X |   |   |  \n",
            "  |   | X |   |  \n",
            "  |   |   | X |  \n",
            "  |   |   |   | G\n",
            "\n",
            "\n",
            "Updated Q-value for state (0, 0), action 'up': -5.126229559267231 -> -5.139861018789744\n",
            "Updated Q-value for state (0, 0), action 'down': -4.736160171658178 -> -4.736247917643473\n",
            "Updated Q-value for state (1, 0), action 'down': -4.15226403501236 -> -4.152328834916348\n",
            "Updated Q-value for state (2, 0), action 'down': -3.5032355933913846 -> -3.503307253452103\n",
            "Updated Q-value for state (3, 0), action 'right': -2.782169104442851 -> -2.8346227989359365\n",
            "Updated Q-value for state (3, 1), action 'left': -2.5630067215263423 -> -2.6571016133793397\n",
            "Updated Q-value for state (3, 0), action 'down': -2.7821729333959047 -> -2.7822412392871967\n",
            "Updated Q-value for state (4, 0), action 'right': -1.9809511025653632 -> -1.9809559925110227\n",
            "Updated Q-value for state (4, 1), action 'right': -1.09000000224662 -> -1.090000002030856\n",
            "Updated Q-value for state (4, 2), action 'right': -0.10000000009886692 -> -0.10000000008942911\n",
            "Updated Q-value for state (4, 3), action 'right': 0.9999999999950124 -> 0.9999999999955111\n",
            "\n",
            "Episode 100: Starting new episode...\n",
            "\n",
            "A |   |   |   |  \n",
            "  | X |   |   |  \n",
            "  |   | X |   |  \n",
            "  |   |   | X |  \n",
            "  |   |   |   | G\n",
            "\n",
            "\n",
            "Updated Q-value for state (0, 0), action 'right': -4.7638086865968186 -> -4.7620805463891065\n",
            "Updated Q-value for state (0, 1), action 'right': -4.162808093910776 -> -4.162988598258234\n",
            "Updated Q-value for state (0, 2), action 'down': -3.5162368193170694 -> -3.5184256978431363\n",
            "Updated Q-value for state (1, 2), action 'right': -2.820139560641933 -> -2.821330797532702\n",
            "Updated Q-value for state (1, 3), action 'down': -2.03561325505514 -> -2.0359880912636092\n",
            "Updated Q-value for state (2, 3), action 'right': -1.1548462412664828 -> -1.1626802505614542\n",
            "Updated Q-value for state (2, 4), action 'down': -0.25909592690688554 -> -0.24318633421619884\n",
            "Updated Q-value for state (3, 4), action 'down': 0.9999999999999792 -> 0.9999999999999813\n",
            "\n",
            "Episode 200: Starting new episode...\n",
            "\n",
            "A |   |   |   |  \n",
            "  | X |   |   |  \n",
            "  |   | X |   |  \n",
            "  |   |   | X |  \n",
            "  |   |   |   | G\n",
            "\n",
            "\n",
            "Updated Q-value for state (0, 0), action 'right': -4.8241710010168655 -> -4.825250384814386\n",
            "Updated Q-value for state (0, 1), action 'right': -4.261072043324521 -> -4.262789135467492\n",
            "Updated Q-value for state (0, 2), action 'down': -3.6424921830602512 -> -3.6401276514321115\n",
            "Updated Q-value for state (1, 2), action 'right': -2.909829851976503 -> -2.9109439632031946\n",
            "Updated Q-value for state (1, 3), action 'down': -2.134412182492689 -> -2.1303979786384493\n",
            "Updated Q-value for state (2, 3), action 'right': -1.2158557155003238 -> -1.205412342746188\n",
            "Updated Q-value for state (2, 4), action 'down': -0.12380220884329479 -> -0.12142198795896536\n",
            "Updated Q-value for state (3, 4), action 'down': 0.9999999999999996 -> 0.9999999999999996\n",
            "\n",
            "Episode 300: Starting new episode...\n",
            "\n",
            "A |   |   |   |  \n",
            "  | X |   |   |  \n",
            "  |   | X |   |  \n",
            "  |   |   | X |  \n",
            "  |   |   |   | G\n",
            "\n",
            "\n",
            "Updated Q-value for state (0, 0), action 'down': -4.865583539698902 -> -4.864975764613475\n",
            "Updated Q-value for state (1, 0), action 'down': -4.288339765382934 -> -4.287878484100289\n",
            "Updated Q-value for state (2, 0), action 'down': -3.648585502840536 -> -3.6453182164501166\n",
            "Updated Q-value for state (3, 0), action 'right': -2.906569598818155 -> -2.9077192121658717\n",
            "Updated Q-value for state (3, 1), action 'right': -2.131184146994806 -> -2.125762132618414\n",
            "Updated Q-value for state (3, 2), action 'down': -1.1966266702565431 -> -1.1896637508782861\n",
            "Updated Q-value for state (4, 2), action 'right': -0.14110830719330283 -> -0.1369974764739726\n",
            "Updated Q-value for state (4, 3), action 'right': 0.9999999999999996 -> 0.9999999999999996\n",
            "\n",
            "Episode 400: Starting new episode...\n",
            "\n",
            "A |   |   |   |  \n",
            "  | X |   |   |  \n",
            "  |   | X |   |  \n",
            "  |   |   | X |  \n",
            "  |   |   |   | G\n",
            "\n",
            "\n",
            "Updated Q-value for state (0, 0), action 'right': -4.896213492900994 -> -4.898502036559008\n",
            "Updated Q-value for state (0, 1), action 'right': -4.3545543660901505 -> -4.350058459373468\n",
            "Updated Q-value for state (0, 2), action 'right': -3.677328109914801 -> -3.6745057249822066\n",
            "Updated Q-value for state (0, 3), action 'down': -2.943449178432065 -> -2.9433916419871027\n",
            "Updated Q-value for state (1, 3), action 'down': -2.1587486822027127 -> -2.1573407625413257\n",
            "Updated Q-value for state (2, 3), action 'right': -1.2718549839876045 -> -1.2724832361502105\n",
            "Updated Q-value for state (2, 4), action 'down': -0.3090416729040713 -> -0.28813750561366425\n",
            "Updated Q-value for state (3, 4), action 'down': 0.9999999999999996 -> 0.9999999999999996\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train with Q-learning TD(2)"
      ],
      "metadata": {
        "id": "Iu8T7snTHJW5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "q_table = initialize_q_table(env)\n",
        "\n",
        "print(\"\\nTraining with Q-learning TD(2):\\n\")\n",
        "q_learning_td2_q_table = q_learning_td2(env, q_table, episodes=500)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z_RozqfLF05C",
        "outputId": "3eac9d8c-4d79-4bb8-dfc2-636e91ebf8c6"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training with Q-learning TD(2):\n",
            "\n",
            "\n",
            "Episode 0: Starting new episode...\n",
            "\n",
            "A |   |   |   |  \n",
            "  | X |   |   |  \n",
            "  |   | X |   |  \n",
            "  |   |   | X |  \n",
            "  |   |   |   | G\n",
            "\n",
            "\n",
            "Updated Q-value for state (0, 0), action 'up': 0 -> -0.19\n",
            "Updated Q-value for state (0, 0), action 'up': -0.19 -> -0.361\n",
            "Updated Q-value for state (0, 0), action 'down': 0 -> -0.19\n",
            "Updated Q-value for state (1, 0), action 'up': 0 -> -0.19\n",
            "Updated Q-value for state (0, 0), action 'left': 0 -> -0.19\n",
            "Updated Q-value for state (0, 0), action 'left': -0.19 -> -0.361\n",
            "Updated Q-value for state (0, 0), action 'right': 0 -> -0.19\n",
            "Updated Q-value for state (0, 1), action 'up': 0 -> -0.19\n",
            "Updated Q-value for state (0, 1), action 'up': -0.19 -> -0.361\n",
            "Updated Q-value for state (0, 1), action 'down': 0 -> -0.19\n",
            "Updated Q-value for state (0, 1), action 'down': -0.19 -> -0.361\n",
            "Updated Q-value for state (0, 1), action 'left': 0 -> -0.20539000000000002\n",
            "Updated Q-value for state (0, 0), action 'down': -0.19 -> -0.361\n",
            "Updated Q-value for state (1, 0), action 'down': 0 -> -0.19\n",
            "Updated Q-value for state (2, 0), action 'left': 0 -> -0.19\n",
            "Updated Q-value for state (2, 0), action 'up': 0 -> -0.19\n",
            "Updated Q-value for state (1, 0), action 'left': 0 -> -0.19\n",
            "Updated Q-value for state (1, 0), action 'left': -0.19 -> -0.361\n",
            "Updated Q-value for state (1, 0), action 'right': 0 -> -0.19\n",
            "Updated Q-value for state (1, 0), action 'right': -0.19 -> -0.37639\n",
            "Updated Q-value for state (1, 0), action 'up': -0.19 -> -0.37639\n",
            "Updated Q-value for state (0, 0), action 'right': -0.19 -> -0.361\n",
            "Updated Q-value for state (0, 1), action 'right': 0 -> -0.19\n",
            "Updated Q-value for state (0, 2), action 'up': 0 -> -0.19\n",
            "Updated Q-value for state (0, 2), action 'up': -0.19 -> -0.361\n",
            "Updated Q-value for state (0, 2), action 'down': 0 -> -0.19\n",
            "Updated Q-value for state (1, 2), action 'up': 0 -> -0.19\n",
            "Updated Q-value for state (0, 2), action 'left': 0 -> -0.20539000000000002\n",
            "Updated Q-value for state (0, 1), action 'right': -0.19 -> -0.361\n",
            "Updated Q-value for state (0, 2), action 'right': 0 -> -0.19\n",
            "Updated Q-value for state (0, 3), action 'up': 0 -> -0.19\n",
            "Updated Q-value for state (0, 3), action 'up': -0.19 -> -0.361\n",
            "Updated Q-value for state (0, 3), action 'down': 0 -> -0.19\n",
            "Updated Q-value for state (1, 3), action 'up': 0 -> -0.19\n",
            "Updated Q-value for state (0, 3), action 'left': 0 -> -0.20539000000000002\n",
            "Updated Q-value for state (0, 2), action 'down': -0.19 -> -0.361\n",
            "Updated Q-value for state (1, 2), action 'down': 0 -> -0.19\n",
            "Updated Q-value for state (1, 2), action 'down': -0.19 -> -0.361\n",
            "Updated Q-value for state (1, 2), action 'left': 0 -> -0.19\n",
            "Updated Q-value for state (1, 2), action 'left': -0.19 -> -0.361\n",
            "Updated Q-value for state (1, 2), action 'right': 0 -> -0.19\n",
            "Updated Q-value for state (1, 3), action 'down': 0 -> -0.19\n",
            "Updated Q-value for state (2, 3), action 'up': 0 -> -0.19\n",
            "Updated Q-value for state (1, 3), action 'left': 0 -> -0.20539000000000002\n",
            "Updated Q-value for state (1, 2), action 'up': -0.19 -> -0.37639\n",
            "Updated Q-value for state (0, 2), action 'right': -0.19 -> -0.361\n",
            "Updated Q-value for state (0, 3), action 'right': 0 -> -0.19\n",
            "Updated Q-value for state (0, 4), action 'up': 0 -> -0.19\n",
            "Updated Q-value for state (0, 4), action 'up': -0.19 -> -0.361\n",
            "Updated Q-value for state (0, 4), action 'down': 0 -> -0.19\n",
            "Updated Q-value for state (1, 4), action 'up': 0 -> -0.19\n",
            "Updated Q-value for state (0, 4), action 'left': 0 -> -0.20539000000000002\n",
            "Updated Q-value for state (0, 3), action 'down': -0.19 -> -0.37639\n",
            "Updated Q-value for state (1, 3), action 'up': -0.19 -> -0.37639\n",
            "Updated Q-value for state (0, 3), action 'right': -0.19 -> -0.361\n",
            "Updated Q-value for state (0, 4), action 'right': 0 -> -0.19\n",
            "Updated Q-value for state (0, 4), action 'right': -0.19 -> -0.37639\n",
            "Updated Q-value for state (0, 4), action 'down': -0.19 -> -0.361\n",
            "Updated Q-value for state (1, 4), action 'down': 0 -> -0.19\n",
            "Updated Q-value for state (2, 4), action 'up': 0 -> -0.19\n",
            "Updated Q-value for state (1, 4), action 'left': 0 -> -0.19\n",
            "Updated Q-value for state (1, 3), action 'right': 0 -> -0.19\n",
            "Updated Q-value for state (1, 4), action 'right': 0 -> -0.19\n",
            "Updated Q-value for state (1, 4), action 'right': -0.19 -> -0.37639\n",
            "Updated Q-value for state (1, 4), action 'up': -0.19 -> -0.37763659000000005\n",
            "Updated Q-value for state (0, 4), action 'left': -0.20539000000000002 -> -0.39148759000000005\n",
            "Updated Q-value for state (0, 3), action 'left': -0.20539000000000002 -> -0.404092\n",
            "Updated Q-value for state (0, 2), action 'up': -0.361 -> -0.53153659\n",
            "Updated Q-value for state (0, 2), action 'left': -0.20539000000000002 -> -0.39148759000000005\n",
            "Updated Q-value for state (0, 1), action 'left': -0.20539000000000002 -> -0.404092\n",
            "Updated Q-value for state (0, 0), action 'up': -0.361 -> -0.544141\n",
            "Updated Q-value for state (0, 0), action 'up': -0.544141 -> -0.7089679\n",
            "Updated Q-value for state (0, 0), action 'down': -0.361 -> -0.53029\n",
            "Updated Q-value for state (1, 0), action 'down': -0.19 -> -0.361\n",
            "Updated Q-value for state (2, 0), action 'down': 0 -> -0.19\n",
            "Updated Q-value for state (3, 0), action 'up': 0 -> -0.19\n",
            "Updated Q-value for state (2, 0), action 'right': 0 -> -0.19\n",
            "Updated Q-value for state (2, 1), action 'up': 0 -> -0.19\n",
            "Updated Q-value for state (2, 1), action 'left': 0 -> -0.20539000000000002\n",
            "Updated Q-value for state (2, 0), action 'up': -0.19 -> -0.390241\n",
            "Updated Q-value for state (1, 0), action 'down': -0.361 -> -0.53029\n",
            "Updated Q-value for state (2, 0), action 'down': -0.19 -> -0.361\n",
            "Updated Q-value for state (3, 0), action 'down': 0 -> -0.19\n",
            "Updated Q-value for state (4, 0), action 'up': 0 -> -0.19\n",
            "Updated Q-value for state (3, 0), action 'left': 0 -> -0.19\n",
            "Updated Q-value for state (3, 0), action 'right': 0 -> -0.19\n",
            "Updated Q-value for state (3, 1), action 'up': 0 -> -0.19\n",
            "Updated Q-value for state (2, 1), action 'down': 0 -> -0.19\n",
            "Updated Q-value for state (3, 1), action 'down': 0 -> -0.19\n",
            "Updated Q-value for state (4, 1), action 'up': 0 -> -0.19\n",
            "Updated Q-value for state (3, 1), action 'left': 0 -> -0.20539000000000002\n",
            "Updated Q-value for state (3, 0), action 'up': -0.19 -> -0.37639\n",
            "Updated Q-value for state (2, 0), action 'left': -0.19 -> -0.37639\n",
            "Updated Q-value for state (2, 0), action 'left': -0.37639 -> -0.544141\n",
            "Updated Q-value for state (2, 0), action 'right': -0.19 -> -0.361\n",
            "Updated Q-value for state (2, 1), action 'right': 0 -> -0.19\n",
            "Updated Q-value for state (2, 1), action 'right': -0.19 -> -0.37639\n",
            "Updated Q-value for state (2, 1), action 'up': -0.19 -> -0.37639\n",
            "Updated Q-value for state (2, 1), action 'up': -0.37639 -> -0.544141\n",
            "Updated Q-value for state (2, 1), action 'down': -0.19 -> -0.361\n",
            "Updated Q-value for state (3, 1), action 'right': 0 -> -0.19\n",
            "Updated Q-value for state (3, 2), action 'up': 0 -> -0.19\n",
            "Updated Q-value for state (3, 2), action 'up': -0.19 -> -0.361\n",
            "Updated Q-value for state (3, 2), action 'down': 0 -> -0.19\n",
            "Updated Q-value for state (4, 2), action 'up': 0 -> -0.19\n",
            "Updated Q-value for state (3, 2), action 'left': 0 -> -0.20539000000000002\n",
            "Updated Q-value for state (3, 1), action 'up': -0.19 -> -0.37763659000000005\n",
            "Updated Q-value for state (2, 1), action 'left': -0.20539000000000002 -> -0.404092\n",
            "Updated Q-value for state (2, 0), action 'down': -0.361 -> -0.53029\n",
            "Updated Q-value for state (3, 0), action 'down': -0.19 -> -0.361\n",
            "Updated Q-value for state (4, 0), action 'down': 0 -> -0.19\n",
            "Updated Q-value for state (4, 0), action 'down': -0.19 -> -0.361\n",
            "Updated Q-value for state (4, 0), action 'left': 0 -> -0.19\n",
            "Updated Q-value for state (4, 0), action 'left': -0.19 -> -0.361\n",
            "Updated Q-value for state (4, 0), action 'right': 0 -> -0.19\n",
            "Updated Q-value for state (4, 1), action 'down': 0 -> -0.19\n",
            "Updated Q-value for state (4, 1), action 'down': -0.19 -> -0.361\n",
            "Updated Q-value for state (4, 1), action 'left': 0 -> -0.20539000000000002\n",
            "Updated Q-value for state (4, 0), action 'up': -0.19 -> -0.37639\n",
            "Updated Q-value for state (3, 0), action 'left': -0.19 -> -0.37639\n",
            "Updated Q-value for state (3, 0), action 'left': -0.37639 -> -0.544141\n",
            "Updated Q-value for state (3, 0), action 'right': -0.19 -> -0.37639\n",
            "Updated Q-value for state (3, 1), action 'down': -0.19 -> -0.361\n",
            "Updated Q-value for state (4, 1), action 'right': 0 -> -0.19\n",
            "Updated Q-value for state (4, 2), action 'down': 0 -> -0.19\n",
            "Updated Q-value for state (4, 2), action 'down': -0.19 -> -0.361\n",
            "Updated Q-value for state (4, 2), action 'left': 0 -> -0.20539000000000002\n",
            "Updated Q-value for state (4, 1), action 'up': -0.19 -> -0.37639\n",
            "Updated Q-value for state (3, 1), action 'right': -0.19 -> -0.361\n",
            "Updated Q-value for state (3, 2), action 'right': 0 -> -0.19\n",
            "Updated Q-value for state (3, 2), action 'right': -0.19 -> -0.37639\n",
            "Updated Q-value for state (3, 2), action 'down': -0.19 -> -0.361\n",
            "Updated Q-value for state (4, 2), action 'right': 0 -> -0.19\n",
            "Updated Q-value for state (4, 3), action 'up': 0 -> -0.19\n",
            "Updated Q-value for state (4, 3), action 'up': -0.19 -> -0.361\n",
            "Updated Q-value for state (4, 3), action 'down': 0 -> -0.19\n",
            "Updated Q-value for state (4, 3), action 'down': -0.19 -> -0.361\n",
            "Updated Q-value for state (4, 3), action 'left': 0 -> -0.20539000000000002\n",
            "Updated Q-value for state (4, 2), action 'up': -0.19 -> -0.37763659000000005\n",
            "Updated Q-value for state (3, 2), action 'left': -0.20539000000000002 -> -0.39148759000000005\n",
            "Updated Q-value for state (3, 1), action 'left': -0.20539000000000002 -> -0.404092\n",
            "Updated Q-value for state (3, 0), action 'down': -0.361 -> -0.53029\n",
            "Updated Q-value for state (4, 0), action 'right': -0.19 -> -0.37639\n",
            "Updated Q-value for state (4, 1), action 'right': -0.19 -> -0.37639\n",
            "Updated Q-value for state (4, 2), action 'right': -0.19 -> -0.181\n",
            "\n",
            "Episode 100: Starting new episode...\n",
            "\n",
            "A |   |   |   |  \n",
            "  | X |   |   |  \n",
            "  |   | X |   |  \n",
            "  |   |   | X |  \n",
            "  |   |   |   | G\n",
            "\n",
            "\n",
            "Updated Q-value for state (0, 0), action 'down': -5.939291724963511 -> -5.964806057835438\n",
            "Updated Q-value for state (1, 0), action 'down': -5.301771671213309 -> -5.327256953163425\n",
            "Updated Q-value for state (2, 0), action 'right': -4.514351223104279 -> -4.566379240407266\n",
            "Updated Q-value for state (2, 1), action 'right': -3.8699153038693224 -> -3.986386913095805\n",
            "Updated Q-value for state (2, 1), action 'right': -3.986386913095805 -> -4.09142995658719\n",
            "Updated Q-value for state (2, 1), action 'left': -3.872614009888452 -> -4.042679333687462\n",
            "Updated Q-value for state (2, 0), action 'down': -4.534897836887102 -> -4.5745780154154\n",
            "Updated Q-value for state (3, 0), action 'down': -3.742839039716155 -> -3.808343288608707\n",
            "Updated Q-value for state (4, 0), action 'left': -3.0838043563477457 -> -3.215212073577139\n",
            "Updated Q-value for state (4, 0), action 'left': -3.215212073577139 -> -3.333782805873023\n",
            "Updated Q-value for state (4, 0), action 'down': -3.0875548105382453 -> -3.2188912691380187\n",
            "Updated Q-value for state (4, 0), action 'down': -3.2188912691380187 -> -3.340074922100862\n",
            "Updated Q-value for state (4, 0), action 'right': -3.1243553071190746 -> -3.178992336559871\n",
            "Updated Q-value for state (4, 1), action 'right': -2.1860809895395548 -> -2.171749702512105\n",
            "Updated Q-value for state (4, 2), action 'right': -0.17625693736426984 -> -0.16863124362784285\n",
            "\n",
            "Episode 200: Starting new episode...\n",
            "\n",
            "A |   |   |   |  \n",
            "  | X |   |   |  \n",
            "  |   | X |   |  \n",
            "  |   |   | X |  \n",
            "  |   |   |   | G\n",
            "\n",
            "\n",
            "Updated Q-value for state (0, 0), action 'right': -6.855755348754742 -> -6.864794145764247\n",
            "Updated Q-value for state (0, 1), action 'left': -6.2298065664812174 -> -6.35287423564\n",
            "Updated Q-value for state (0, 0), action 'right': -6.864794145764247 -> -6.872984647768152\n",
            "Updated Q-value for state (0, 1), action 'right': -6.230492797288023 -> -6.239422471976116\n",
            "Updated Q-value for state (0, 2), action 'right': -5.456530301443148 -> -5.468766267646195\n",
            "Updated Q-value for state (0, 3), action 'up': -4.541839461078541 -> -4.6455445113180485\n",
            "Updated Q-value for state (0, 3), action 'up': -4.6455445113180485 -> -4.758342516815522\n",
            "Updated Q-value for state (0, 3), action 'left': -4.782129094188624 -> -4.936481035399801\n",
            "Updated Q-value for state (0, 2), action 'down': -5.463763588025174 -> -5.486327037949035\n",
            "Updated Q-value for state (1, 2), action 'right': -4.6782692435355475 -> -4.6895571133831195\n",
            "Updated Q-value for state (1, 3), action 'right': -3.569318446927495 -> -3.576949983550024\n",
            "Updated Q-value for state (1, 4), action 'down': -2.155103473028129 -> -2.1436666726454687\n",
            "Updated Q-value for state (2, 4), action 'down': -0.17374749284138816 -> -0.16637274355724935\n",
            "\n",
            "Episode 300: Starting new episode...\n",
            "\n",
            "A |   |   |   |  \n",
            "  | X |   |   |  \n",
            "  |   | X |   |  \n",
            "  |   |   | X |  \n",
            "  |   |   |   | G\n",
            "\n",
            "\n",
            "Updated Q-value for state (0, 0), action 'down': -7.156849152076775 -> -7.160326842125347\n",
            "Updated Q-value for state (1, 0), action 'down': -6.53287166983023 -> -6.537076661471841\n",
            "Updated Q-value for state (2, 0), action 'down': -5.77150813116832 -> -5.775179549032917\n",
            "Updated Q-value for state (3, 0), action 'right': -4.824965814585555 -> -4.832375005585487\n",
            "Updated Q-value for state (3, 1), action 'right': -3.7025404007220573 -> -3.699860688299186\n",
            "Updated Q-value for state (3, 2), action 'down': -2.1922756499917835 -> -2.177868007026431\n",
            "Updated Q-value for state (4, 2), action 'right': -0.18296200041760324 -> -0.17466580037584292\n",
            "\n",
            "Episode 400: Starting new episode...\n",
            "\n",
            "A |   |   |   |  \n",
            "  | X |   |   |  \n",
            "  |   | X |   |  \n",
            "  |   |   | X |  \n",
            "  |   |   |   | G\n",
            "\n",
            "\n",
            "Updated Q-value for state (0, 0), action 'down': -7.267085378822088 -> -7.267998949787462\n",
            "Updated Q-value for state (1, 0), action 'down': -6.637309985772635 -> -6.637342807878772\n",
            "Updated Q-value for state (2, 0), action 'down': -5.8489360578197545 -> -5.849581595027028\n",
            "Updated Q-value for state (3, 0), action 'right': -4.883199296163569 -> -4.918870070102944\n",
            "Updated Q-value for state (3, 1), action 'up': -4.123342019206568 -> -4.296653989532633\n",
            "Updated Q-value for state (2, 1), action 'down': -4.88452064502125 -> -4.883657036911735\n",
            "Updated Q-value for state (3, 1), action 'right': -3.6739315604026035 -> -3.677606823611804\n",
            "Updated Q-value for state (3, 2), action 'down': -2.235412583326679 -> -2.2194200897510834\n",
            "Updated Q-value for state (4, 2), action 'right': -0.2166514167539798 -> -0.20498627507858183\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sarsa_td0_q_table = sarsa_td2(env, q_table, episodes=500)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Kq5_2L7F7yR",
        "outputId": "3f4d8da5-79c0-45c6-9c51-baf7244dad30"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Episode 0: Starting new episode...\n",
            "\n",
            "A |   |   |   |  \n",
            "  | X |   |   |  \n",
            "  |   | X |   |  \n",
            "  |   |   | X |  \n",
            "  |   |   |   | G\n",
            "\n",
            "\n",
            "Updated Q-value for state (0, 0), action 'down': -7.298327537545865 -> -7.23445142003769\n",
            "Updated Q-value for state (1, 0), action 'down': -6.659225772551028 -> -6.604616236830273\n",
            "Updated Q-value for state (2, 0), action 'right': -5.876007854893968 -> -5.954363705650982\n",
            "Updated Q-value for state (2, 1), action 'left': -5.2013955744981155 -> -5.269264363003364\n",
            "Updated Q-value for state (2, 0), action 'right': -5.954363705650982 -> -5.849841597563477\n",
            "Updated Q-value for state (2, 1), action 'down': -4.9136832833958 -> -4.791271890098189\n",
            "Updated Q-value for state (3, 1), action 'down': -3.7149908947851102 -> -3.560640328102451\n",
            "Updated Q-value for state (4, 1), action 'right': -2.209344877061347 -> -2.178410389355212\n",
            "Updated Q-value for state (4, 2), action 'right': -0.3351669480969355 -> -0.31165025328724194\n",
            "\n",
            "Episode 100: Starting new episode...\n",
            "\n",
            "A |   |   |   |  \n",
            "  | X |   |   |  \n",
            "  |   | X |   |  \n",
            "  |   |   | X |  \n",
            "  |   |   |   | G\n",
            "\n",
            "\n",
            "Updated Q-value for state (0, 0), action 'down': -5.308555132365331 -> -5.290584663791762\n",
            "Updated Q-value for state (1, 0), action 'down': -5.028515754049848 -> -5.017293204379648\n",
            "Updated Q-value for state (2, 0), action 'right': -3.9862351192958507 -> -4.122140409258959\n",
            "Updated Q-value for state (2, 1), action 'down': -3.7238151325282094 -> -3.8430626450101735\n",
            "Updated Q-value for state (3, 1), action 'up': -4.253441998675225 -> -4.212298846534785\n",
            "Updated Q-value for state (2, 1), action 'down': -3.8430626450101735 -> -3.928904983967742\n",
            "Updated Q-value for state (3, 1), action 'down': -2.3975437990997843 -> -2.5488830150974255\n",
            "Updated Q-value for state (4, 1), action 'left': -3.458624734056615 -> -3.4598694337479095\n",
            "Updated Q-value for state (4, 0), action 'right': -2.48263698651382 -> -2.434853954405071\n",
            "Updated Q-value for state (4, 1), action 'right': -1.939594729592049 -> -1.935635256632844\n",
            "Updated Q-value for state (4, 2), action 'right': -0.12939094497077727 -> -0.12645185047369956\n",
            "\n",
            "Episode 200: Starting new episode...\n",
            "\n",
            "A |   |   |   |  \n",
            "  | X |   |   |  \n",
            "  |   | X |   |  \n",
            "  |   |   | X |  \n",
            "  |   |   |   | G\n",
            "\n",
            "\n",
            "Updated Q-value for state (0, 0), action 'down': -5.406137710710796 -> -5.376954916495013\n",
            "Updated Q-value for state (1, 0), action 'down': -5.1135503471268535 -> -5.214932250557265\n",
            "Updated Q-value for state (2, 0), action 'right': -3.9682836648802007 -> -4.082886275247477\n",
            "Updated Q-value for state (2, 1), action 'left': -5.218974544976502 -> -5.174638988622781\n",
            "Updated Q-value for state (2, 0), action 'right': -4.082886275247477 -> -4.0394631792823565\n",
            "Updated Q-value for state (2, 1), action 'down': -3.5501468906657827 -> -3.5409752515070116\n",
            "Updated Q-value for state (3, 1), action 'down': -2.158833722958366 -> -2.147197604918565\n",
            "Updated Q-value for state (4, 1), action 'right': -1.9239882704667575 -> -2.030745964722997\n",
            "Updated Q-value for state (4, 2), action 'right': -0.17589202785228852 -> -0.362550079323095\n",
            "Updated Q-value for state (4, 3), action 'left': -1.3476113741100666 -> -1.4028502366990598\n",
            "Updated Q-value for state (4, 2), action 'right': -0.362550079323095 -> -0.3362950713907855\n",
            "\n",
            "Episode 300: Starting new episode...\n",
            "\n",
            "A |   |   |   |  \n",
            "  | X |   |   |  \n",
            "  |   | X |   |  \n",
            "  |   |   | X |  \n",
            "  |   |   |   | G\n",
            "\n",
            "\n",
            "Updated Q-value for state (0, 0), action 'down': -5.207244638855546 -> -5.3146491744430255\n",
            "Updated Q-value for state (1, 0), action 'down': -4.920423632794543 -> -4.930062555386652\n",
            "Updated Q-value for state (2, 0), action 'left': -5.408999993494254 -> -5.354907731682991\n",
            "Updated Q-value for state (2, 0), action 'right': -3.8479171095254685 -> -3.851896001809308\n",
            "Updated Q-value for state (2, 1), action 'down': -3.664293056026707 -> -3.6496558885339287\n",
            "Updated Q-value for state (3, 1), action 'down': -2.45395806464674 -> -2.43071259982948\n",
            "Updated Q-value for state (4, 1), action 'right': -1.9974338038258315 -> -1.9876904234432482\n",
            "Updated Q-value for state (4, 2), action 'right': -0.39691779811622474 -> -0.3672260183046023\n",
            "\n",
            "Episode 400: Starting new episode...\n",
            "\n",
            "A |   |   |   |  \n",
            "  | X |   |   |  \n",
            "  |   | X |   |  \n",
            "  |   |   | X |  \n",
            "  |   |   |   | G\n",
            "\n",
            "\n",
            "Updated Q-value for state (0, 0), action 'down': -5.294368992134026 -> -5.380838347910029\n",
            "Updated Q-value for state (1, 0), action 'down': -4.856598174001336 -> -4.866111357528192\n",
            "Updated Q-value for state (2, 0), action 'left': -5.2581019134494404 -> -5.217749645317928\n",
            "Updated Q-value for state (2, 0), action 'right': -3.7675679126788872 -> -3.7642467466774505\n",
            "Updated Q-value for state (2, 1), action 'down': -3.6476286816473134 -> -3.6367279652157083\n",
            "Updated Q-value for state (3, 1), action 'down': -2.2646373489685425 -> -2.3760149570382696\n",
            "Updated Q-value for state (4, 1), action 'right': -2.0229895275694623 -> -2.033775877912358\n",
            "Updated Q-value for state (4, 2), action 'down': -1.825201765019523 -> -1.8326815885175707\n",
            "Updated Q-value for state (4, 2), action 'right': -0.28500374197335604 -> -0.26650336777602046\n"
          ]
        }
      ]
    }
  ]
}
